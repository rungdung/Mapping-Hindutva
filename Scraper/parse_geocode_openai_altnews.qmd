```{python}
import pandas as pd
import json
import re

# Load CSV
file_path = "/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_13_03_2025.csv"
output_filepath = "/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_13_03_2025_openaibatch.jsonl"

db = pd.read_csv(file_path)

# Drop duplicate entries based on 'id'
db = db.drop_duplicates(subset='id')

# Clean and sanitize content
escapes = ''.join([chr(char) for char in range(1, 32)])
translator = str.maketrans('', '', escapes)
db['content'] = db['content'].apply(lambda x: x.translate(translator) if isinstance(x, str) else x)

# Write to JSONL file
with open(output_filepath, 'w', encoding='utf-8') as f:
    for row in db.itertuples():
        record = {
            "custom_id": f"{row.id}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-4o-mini",
                "messages": [
                    {
                        "role": "system",
                        "content": (
                            "You are going to be fed news articles about events mostly in India. "
                            "You must detect if it is about a real event, opinion article, or an event in the past. "
                            "You must then detect the locations pertinent to that event (with the place name, district, state if available) "
                            "(if it is multiple, list all event locations in [exact point of interest, place name, district, state] format), "
                            "the date, and involved entities."
                        )
                    },
                    {
                        "role": "user",
                        "content": f"{row.title} {row.excerpt}"
                    }
                ],
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                        "name": "event",
                        "strict": True,
                        "schema": {
                            "$defs": {
                                "Location": {
                                    "properties": {
                                        "address": {"title": "Address", "type": "string"},
                                        "district": {"title": "District", "type": "string"},
                                        "state": {"title": "State", "type": "string"},
                                        "country": {"title": "Country", "type": "string"}
                                    },
                                    "required": ["address", "district", "state", "country"],
                                    "additionalProperties": False,
                                    "title": "Location",
                                    "type": "object"
                                }
                            },
                            "properties": {
                                "location": {
                                    "items": {"$ref": "#/$defs/Location"},
                                    "title": "Location",
                                    "type": "array"
                                },
                                "dateOfEvent": {"title": "DateOfEvent", "type": "string"},
                                "involvedGroups": {
                                    "items": {"type": "string"},
                                    "title": "InvolvedGroups",
                                    "type": "array"
                                },
                                "typeOfEvent": {"title": "TypeOfEvent", "type": "string"}
                            },
                            "additionalProperties": False,
                            "required": ["location", "dateOfEvent", "involvedGroups", "typeOfEvent"],
                            "title": "HindutvaEvent",
                            "type": "object"
                        }
                    }
                }
            }
        }

        json.dump(record, f)  # Write as valid JSON
        f.write("\n")  # Ensure newline for JSONL format

# Validate JSONL file
valid_lines = []
with open(output_filepath, 'r', encoding='utf-8') as f:
    for line in f:
        try:
            json.loads(line.strip())  # Validate JSON
            valid_lines.append(line.strip())
        except json.JSONDecodeError as e:
            print(f"Skipping invalid JSONL entry: {e}")

# Save only valid JSONL entries
with open(output_filepath, 'w', encoding='utf-8') as f:
    for line in valid_lines:
        f.write(line + "\n")

# Final validation
try:
    with open(output_filepath, 'r', encoding='utf-8') as file:
        for line in file:
            json.loads(line)
    print("Valid JSONL file")
except json.JSONDecodeError as e:
    print(f"Invalid JSONL: {e}")
except Exception as e:
    print(f"Error reading file: {e}")

```

```{python}
#upload

from openai import OpenAI
import pandas as pd
import json
import os

OPENAI_API_KEY = import.env("OPENAI_API_KEY")
filepath = '/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_13_03_2025_openaibatch.jsonl'

import os
from openai import OpenAI

client = OpenAI(api_key=OPENAI_API_KEY)

filepath = "/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_13_03_2025_openaibatch.jsonl"
batch_dir = "/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/openai_batch/"

# Ensure the batch directory exists
os.makedirs(batch_dir, exist_ok=True)

# Split the file into 1000-line chunks and save as individual files
with open(filepath, 'r', encoding='utf-8') as f_read:
    lines = f_read.readlines()
    for i in range(0, len(lines), 1000):
        try:
            chunk_path = f"{batch_dir}altnews_openaibatch_{i}.jsonl"
            with open(chunk_path, 'w', encoding='utf-8') as f_write:
                f_write.writelines(lines[i:i+1000])
        except Exception as e:
            print(f"Error writing chunk {i}: {e}")

# Process batch files
batch_files = [f for f in os.listdir(batch_dir) if os.path.isfile(os.path.join(batch_dir, f))]

for file in batch_files:
    file_path = os.path.join(batch_dir, file)
    
    try:
        batch_input_file = client.files.create(
            file=open(file_path, "rb"),
            purpose="batch"
        )
        
        batch_input_file_id = batch_input_file.id
        print(f"Batch File ID: {batch_input_file_id}")

        client.batches.create(
            input_file_id=batch_input_file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={
                "description": file
            }
        )

    except Exception as e:
        print(f"Error processing {file}: {e}")

## test



```



```{python}
import ast
import pandas as pd

# read all json files in dir
dir = '/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/openai_batch/output'

json_files = [f for f in os.listdir(dir) if f.endswith('.jsonl')]

df_list = []
for f in json_files:
    df_list.append(pd.read_json(f'{dir}/{f}', lines=True))

df = pd.concat(df_list)

# preserve only unique custom_id
df = df.drop_duplicates(subset=['custom_id'])

df_exploded = []

def checkIntorStr(x):
    try:
        x = int(x)
        return x
    except:
        return None
# flatten the dataframe
for row in df.to_dict(orient="records"):
    content = ast.literal_eval(row['response']['body']['choices'][0]['message']['content'])
    new_row = {
        'openai_id': row['id'],
        'id': checkIntorStr(row['custom_id']),
        'locations': content['location'],
        'date': content['dateOfEvent'],
        'involved_groups': content['involvedGroups']
    }

    df_exploded.append(new_row)

df_exploded_full = pd.DataFrame(df_exploded)
# drop all rows with no id
df_exploded_full.dropna(subset=['id'], inplace=True)

# join with the original dataframe
df_og = pd.read_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_13_03_2025.csv')

df_og['id'] = df_og['id'].apply(lambda x: checkIntorStr(x))
df_og.dropna(subset=['id'], inplace=True)
df_og['id'] = df_og['id'].astype(int)
df_exploded_full['id'] = df_exploded_full['id'].astype(int)

# get unique list of ids and see how many match
exploded_ids = df_exploded_full['id'].unique()
og_ids = df_og['id'].unique()
print(len(exploded_ids))
print(len(og_ids))
# remove ids that are not in both lists
missing_ids = set(og_ids) - set(exploded_ids)

df_og.set_index('id', inplace=True)
df_exploded_full.set_index('id', inplace=True)
df_joined = df_exploded_full.join(df_og, on='id', how='left', lsuffix='_openai', rsuffix='_altnews')


# # count null values
# df_joined.dropna(subset=['locations'], inplace=True)
# remove one id col

# select columns to keep
df_joined_ = df_joined[['locations', 'date_openai', 'involved_groups',
       'date_altnews', 'date_gmt', 'guid',
       'modified', 'modified_gmt', 'slug', 'link', 'title',
       'content', 'excerpt', 'author', 'featured_media', 'coauthors', 'class_list']]

def getCategoriesFromClassList(class_list):
    parsed = ast.literal_eval(class_list)
    final_list = []
    # return only if the word has category or fact in it
    for word in parsed:
        if 'category' in word or 'fact' in word:
            final_list.append(word)
    print(final_list)
    return final_list

df_joined_['categories'] = df_joined_['class_list'].apply(lambda x: getCategoriesFromClassList(x))

df_joined_.drop('class_list', axis=1, inplace=True)
df_joined_.to_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_geolocated_13_03_2025.csv', index=True)
```


# geocode all unique locations
```{python}
import pandas as pd
import ast
import time
import logging
logging.basicConfig(level=logging.DEBUG)
from geopy.geocoders import GoogleV3
import geopandas as gpd
mapsapi = import.env("GOOGLE_MAPS_API_KEY")

df = pd.read_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_geolocated_13_03_2025.csv')

geolocator = GoogleV3(api_key=mapsapi)

def getNaturalLocations(x):
    listLocations = []
    try:
        locations = ast.literal_eval(x)
        for location in locations:
            # enter the object and get only the values of the key:value pair
            location_str = str(location['address'] + ', ' + location['district'] + ', ' + location['state'] + ', ' + location['country'])
            location_str.replace("'", "")
            print(location_str)
            listLocations.append(location_str)
        return listLocations
    except:
        return None

df['natural_locations_openai'] = df['locations'].apply(lambda x: getNaturalLocations(x))

# explode 
df_ = df.explode('natural_locations_openai')

# clean text
def clean_text(text):
    # check if string
    try:
        text = str(text)
        # clean out N/A and Not available
        if "N/A" in text:
            return None
        if "Not available" in text:
            return None
        if "Not specified" in text:
            return None
        # if less than 5 chars or empty
        if "nan" in text:
            return None
        return text
    except:
        return None
    

df_['natural_locations_openai'] = df_['natural_locations_openai'].apply(lambda x: clean_text(x))
df_.dropna(subset=['natural_locations_openai'], inplace=True)


# get unique list of locations
list_locations = df_['natural_locations_openai'].unique()

# save 
with open('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/unique_locations_openai.txt', 'w') as f:
    f.write('\n'.join(list_locations))

# loop over all locations
# make df
list_locations_geocoded = []

for location in list_locations[len(list_locations_geocoded):]:
    try:
        new_row = {
            'location': location,
            'coords': geolocator.geocode(location)[1]
        }
        list_locations_geocoded.append(new_row)
        print(new_row)
    except:
        print("Could not geocode location: " + location)
    time.sleep(0.3)

df_locations = pd.DataFrame(list_locations_geocoded)
df_locations.to_csv('./data/unique_locations_openai_geocoded.csv', index=False)
```

```{python}
import pandas as pd
import ast
import json
import pyarrow
import geopandas as gpd
df_locations = pd.read_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/unique_locations_openai_geocoded.csv')
df = pd.read_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_geolocated_13_03_2025.csv')
def getNaturalLocations(x):
    listLocations = []
    try:
        locations = ast.literal_eval(x)
        for location in locations:
            # enter the object and get only the values of the key:value pair
            location_str = str(location['address'] + ', ' + location['district'] + ', ' + location['state'] + ', ' + location['country'])
            location_str.replace("'", "")
            print(location_str)
            listLocations.append(location_str)
        return listLocations
    except:
        return None


def get_coords(x):
    try:
        return ast.literal_eval(df_locations.loc[df_locations['location'] == x, 'coords'].values[0])
    except:
        print(x)
        return None

    
df['natural_locations_openai'] = df['locations'].apply(lambda x: getNaturalLocations(x))
# explode 
df_ = df.explode('natural_locations_openai')

df_['coords'] = df_['natural_locations_openai'].apply(lambda x: get_coords(x))
print(df_['coords'].isna().sum())

#rename col
df_.rename(columns={'date_altnews': 'date'}, inplace=True)

#parse objects and get value directly
df_['content'] = df_['content'].apply(lambda x: ast.literal_eval(x)['rendered'])
df_['title'] = df_['title'].apply(lambda x: ast.literal_eval(x)['rendered'])
df_['excerpt'] = df_['excerpt'].apply(lambda x: ast.literal_eval(x)['rendered'])
df_.to_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_13_03_2025_geocoded.csv', index=False)
df_.to_parquet('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_13_03_2025_geocoded.parquet')
df_.dropna(subset=['coords'], inplace=True)
# parse as geopandas
df_geo = gpd.GeoDataFrame(df_, geometry=gpd.points_from_xy(df_['coords'].str[1].astype(float), df_['coords'].str[0].astype(float)))
# df_geo.to_csv('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_13_03_2025_geocoded.csv', index=False)
# drop needless cols
df_geo.drop(columns=['coords', 'content'], inplace=True)
df_geo.to_file('/mnt/vault/Vault/Projects/Hindutva-Watch-Prototype/Scraper/data/altnews/altnews_openai_13_03_2025_geocoded.geojson', driver='geojson')

```