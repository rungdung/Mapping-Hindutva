```{python}
import requests
import pandas as pd
import matplotlib.pyplot as plt
link = "https://hindutvawatch.org/wp-json/wp/v2/categories?_embed&per_page=100&page=1"

# scrape all categories into a table with id as the primary id

response = requests.get(link)
data = response.json()
df = pd.DataFrame(data)
df.to_csv('./data/categories.csv')

# merge with the rest of the data

file = pd.read_csv('./data/HWdb_23_09_2024.csv')

def numberCleanup(x):
    try: 
        x = x.strip('[]')
        x = int(x)
    except:
        x = 0
    return x

file['categories'] = file['categories'].apply(lambda x: numberCleanup(x))



df = df[['id', 'slug']]
df = df.rename(columns={'slug': 'category_slug'})
file = file.merge(df, left_on='categories', right_on='id', how='left')
file.to_csv('./data/HWdb_23_09_2024.csv')
# visualise count by category
file = pd.read_csv('./data/HWdb_23_09_2024.csv')
file['category_slug'].value_counts().plot(kind='bar')


```

```{python}
import pandas as pd
import polars as pl

file_pl = pl.read_csv('./data/HWdb_23_09_2024.csv', ignore_errors=True, truncate_ragged_lines=True)

# select cols 
file_pl = file_pl.select(
    pl.col('id'),
    pl.col('title.rendered').alias('title'),
    pl.col('categories').alias('categories'),
    pl.col('content.rendered').alias('content'),
    pl.col('excerpt.rendered').alias('excerpt'),
    pl.col('date'),
)

# drop rows with null values
file_pl = file_pl.drop_nulls()

file_pl.write_csv('./data/HWdb_23_09_2024_cleaned.csv')
```



```{python}
import spacy
from spacy import displacy
import csv
import pandas as pd 
# import geopandas as gpd 
# import geopy 
# import matplotlib.pyplot as plt
# from geopy.extra.rate_limiter import RateLimiter
import concurrent.futures

nlp = spacy.load('en_core_web_trf')

# # Set up geolocator
# geolocator = geopy.geocoders.Nominatim(user_agent="HWdbScraper")

file = pd.read_csv('./data/HWdb_23_09_2024_cleaned.csv')
file['geocoded'] = ''
file['coords'] = ''

# Define a function to process each document
def process_doc(doc, index):
    smallest_location = None
    for ent in doc.ents:
        # Spacy NER Named Entity Recognition tags

        # rank NER tags, based on location
        if (ent.label_ == 'LOC' or ent.label_ == 'GPE'):
            displacy.serve(doc, style="ent")
            print(ent.text)
            # location = geolocator.geocode(ent.text, timeout=4)
            # if location is not None:
            #     if smallest_location is None or (location.raw.get('importance', 0) < smallest_location.raw.get('importance', 0)):
            #         smallest_location = location
            # if smallest_location is not None:
            #     file.at[index, 'geocoded'] = smallest_location
            #     # Create point geometry
            #     point = gpd.points_from_xy([smallest_location.longitude], [smallest_location.latitude])
            #     file.at[index, 'coords'] = point
            #     print(str(index) + " Smallest location named "+ smallest_location.address + " geocoded to " + str(smallest_location))

# Use nlp.pipe to process the documents in parallel
docs = list(nlp.pipe(file['excerpt.rendered'].astype('str')))
with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
    futures = [executor.submit(process_doc, doc, index) for index, doc in enumerate(docs)]

# Write the updated DataFrame to a file
file.to_csv('./Scraper/data/HWdb_2024_geocoded.csv', index=False)



```

## Spatially detect

```{python}
import torch
import spacy
import pandas as pd
import re
from bs4 import BeautifulSoup

torch.cuda.empty_cache()

#require gpu
# spacy.require_gpu()
# use cpu


file = pd.read_csv('./Scraper/data/HWdb_23_09_2024.csv')

#clean up column names

file = file.rename(columns={'title.rendered': 'title', 'excerpt.rendered': 'excerpt', 'content.rendered': 'content'})

def clean_text(text):
    try:
        text = BeautifulSoup(text, "html.parser").text
        text = text.replace("\n", " ")
        text = re.sub(r'http\S+', '', text)
        return text
    except:
        return ''

file['content'] = file['content'].apply(lambda x: clean_text(x))

def process_document(doc):
    locations, groups = [], []
    for ent in doc.ents:
        if ent.label_ in ['LOC', 'GPE']:
            locations.append(ent.text)
        if ent.label_ in ['NORP', 'FAC', 'ORG']:
            groups.append(ent.text)
    return locations, groups

#use cpu
nlp = spacy.load('en_core_web_trf', exclude=['tok2vec', 'tagger'])

# nlp = spacy_sentence_bert.load_model('en_roberta_large_nli_stsb_mean_tokens')

locGlobal, groupGlobal = [], []
for doc in nlp.pipe(file['content'], batch_size=1):
    locations, groups = process_document(doc)
    locGlobal.append(locations)
    groupGlobal.append(groups)
    print(len(locGlobal))

    torch.cuda.empty_cache()
    # clear gpu memory

    
file['content.locations'] = locGlobal
file['content.groups'] = groupGlobal
file.to_csv('./Scraper/data/HWdb_23_09_2024_geocoded.csv', index=False)
```

## clean

```{python}
import pandas as pd

db = pd.read_csv('./data/HWdb_23_09_2024_geocoded.csv')

## drop 
db = db.drop(columns=[ 'id_y'])
db = db.dropna(subset=['content'])

#write 
db.to_csv('./data/HWdb_23_09_2024_geocoded.csv', index=False)

```


```{python}
import torch
from transformers import AutoModelForTokenClassification, AutoTokenizer


# Model name from Hugging Face model hub
model_name = "zekun-li/geolm-base-toponym-recognition"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Example input sentence
input_sentence = "Minneapolis, officially the City of Minneapolis, is a city in the state of Minnesota and the county seat of Hennepin County."

# Tokenize input sentence
tokens = tokenizer.encode(input_sentence, return_tensors="pt")

# Pass tokens through the model
outputs = model(tokens) 

# Retrieve predicted labels for each token
predicted_labels = torch.argmax(outputs.logits, dim=2)

predicted_labels = predicted_labels.detach().cpu().numpy()

# Decode predicted labels
predicted_labels = [model.config.id2label[label] for label in predicted_labels[0]]

# Print predicted labels
print(predicted_labels)
```

```{python}
from geograpy import extraction
import pandas as pd
import ast
from rapidfuzz import process, fuzz, utils

file = pd.read_csv('./Scraper/data/HWdb_23_09_2024_geocoded.csv')

# remove null locations
file = file.dropna(subset=['content.locations'])

# get a list of unique locations, after unnesting the list
locations = file['content.locations'].apply(lambda x: ast.literal_eval(x)).explode().unique()

# fuzzy similarity unique
def get_fuzzily_similar_locations(location):
    return process.extract(location, locations, scorer=fuzz.WRatio, score_cutoff=80, processor=utils.default_process, limit=100)


# loop through every list element in the list in locations
file['locations'] = file['locations'].apply(lambda x: get_geoEntities(x))
```

## Detect dates and temporally link
```{python}

import pandas as pd
import spacy

file = pd.read_csv('./data/HWdb_2024_geocoded.csv')

#require gpu
spacy.require_gpu()


def process_document(doc):
    for ent in doc.ents:
        if ent.label_ == 'DATE':
            print(ent.text)

nlp = spacy.load('en_core_web_trf', exclude=['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer'])

dates = []
for doc in nlp.pipe(file['excerpt'], batch_size=5):
    dates = process_document(doc)
```


```{python}
file = pd.read_csv('./Scraper/data/HWdb_23_09_2024_geocoded.csv')

# read dates and get unique dates
# limit to only date and not time
file['dateTime'] = pd.to_datetime(file['date'], format='mixed', errors="coerce")

# get only date
file['date'] = file['dateTime'].dt.date


```

### Groups

```{python}
import ast

file = pd.read_csv('./Scraper/data/HWdb_23_09_2024_geocoded.csv')

uniqueGroups = file['content.groups'].apply(lambda x: ast.literal_eval(x)).explode().unique()

uniqueGroups = pd.Series(uniqueGroups).apply(lambda x: str(x).lower())

uniqueGroups = set(uniqueGroups)
```

## parse geocode using openai
